"""
This script schedules and runs the anomaly detection pipeline. It takes the 
time series csv generated by the object detection pipeline, aggregates the
data, performs time series analysis for anomaly detection, and generates
csv files for visualization.

All parameter values are read from a configuration file named `config.json`.
"""
# Imports from standard library
import json
import os

# Imports from project packages
from common_utils.file_manager import update_relative_paths
from common_utils.scheduler import schedule_and_run_processes
from anomaly_detection.pipeline import AnomalyDetectionPipeline


# Define path-related constants
DATA_INGESTION_DIR = os.path.dirname(os.path.dirname(__file__))
ROOT_DIR = os.path.dirname(DATA_INGESTION_DIR)
CONFIG_PATH = os.path.join(DATA_INGESTION_DIR, 'config.json')


def main():
    """
    Main function to run the anomaly detection pipeline.
    """
    # Load configuration
    with open(CONFIG_PATH, 'r', encoding='utf-8') as config_file:
        config = json.load(config_file)

    download_interval_hours = config['DOWNLOAD_INTERVAL_HOURS']
    sleep_seconds = config['SLEEP_SECONDS']

    anomaly_detection_params = config['anomaly_detection_params']
    anomaly_detection_params = update_relative_paths(anomaly_detection_params, ROOT_DIR)
    anomaly_detection = AnomalyDetectionPipeline(
        **anomaly_detection_params
    )

    list_process = [anomaly_detection.generate_plot_csv]
    schedule_and_run_processes(download_interval_hours, sleep_seconds, list_process)


if __name__ == '__main__':
    main()
